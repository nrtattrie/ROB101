{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bdbb34f3c5ed9115aab5d32e21cc736",
     "grade": false,
     "grade_id": "cell-3ad5bce4a2ae8319",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "University of Michigan - ROB 101 Computational Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "540854cc52bf074b676c07fef892d0be",
     "grade": false,
     "grade_id": "cell-3c9d2b5410cd109a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 9.  Optimization\n",
    "### Due: 11/19 at 9 PM Eastern\n",
    "\n",
    "#### Purpose: Learn how to optimize functions using julia.\n",
    "- Skills\n",
    "    - Computing a Gradient of a function\n",
    "    - Computing the Hessian of a function\n",
    "    - Finding Local minima and maxima of functions\n",
    "- Knowledge\n",
    "    - Using the SimPy package to do symbolic math for you. All those messy derivatives\n",
    "      in Calculus that a person learns to due by hand, a computer can also do, \n",
    "      and without mistakes. \n",
    "    - Understand the significance of minima and maxima in different contexts\n",
    " \n",
    "    \n",
    "#### Task:\n",
    "Complete and run the cells below as directed.\n",
    "\n",
    "### Problem 1:  Calculate the gradient of f(x, y, z) at (1, 2, 3) using a symmetric differences method.\n",
    "Use a step-size h of 0.001.  Your answer should be a 3d vector.\n",
    "## $$ f(x, y, z) = xye^z - 14yz +sin(x)cos(y) -e^{tan(z/y)}$$\n",
    "\n",
    "An example of how to use symmetric difference method to find the gradient at point (1, 2, 3) in the x-direction is as follows: $$ \\frac{\\delta f(x, y, z)} {\\delta x} = \\frac {f(x+h, y, z) - f(x-h, y, z)}{2h} $$\n",
    "You will need to find the gradient in all three directions(x, y, and z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba70f0475884c6b944b569b0c95950b8",
     "grade": false,
     "grade_id": "cell-420c4c26f3135112",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#f is declared for you.  Use step size h, and starting point (x0, y0, z0)\n",
    "f(x, y, z) = x*y*exp(z) - 14y*z + sin(x)*cos(y) - exp((tan(z/y)))\n",
    "h = 0.001\n",
    "(x0, y0, z0) = (-1, -1.1, 1)\n",
    "\n",
    "#first, calculate df/dx \n",
    "# your code here\n",
    "throw(ErrorException())\n",
    "\n",
    "#calculate df/dy \n",
    "# your code here\n",
    "throw(ErrorException())\n",
    "\n",
    "#calculate df/dz \n",
    "# your code here\n",
    "throw(ErrorException())\n",
    "\n",
    "#Create the gradient vector, and save it as gradVect\n",
    "# your code here\n",
    "throw(ErrorException())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddf820574b2f445edd0db09cae5fc167",
     "grade": true,
     "grade_id": "cell-c78802e00ec58b0b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#autograder cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1355bf4fbc7eedcaea5853e827d5e75d",
     "grade": false,
     "grade_id": "cell-0be3c78893e31a5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2:  Solving for the gradient analytically with SymPy\n",
    "The Trick:  There is a python package that we can use in julia called SymPy.  It includes a lot of symbolic math functions, but there is one in particular we are interested in today:\n",
    "\n",
    "The diff() function. It can calculate analytical forms for functions that would frighten\n",
    "most humans! But not you, a denizen of ROB 101. You know that Julia is your friend when\n",
    "you are in need! [Truth in advertising: MATLAB does symbolic math too!]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec4ec4b8c3775ae8b0bc70eeefee0475",
     "grade": false,
     "grade_id": "cell-e6d2cd3f9e84f8a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#an example\n",
    "#using Pkg\n",
    "#Pkg.add(\"SymPy\")\n",
    "using SymPy\n",
    "@vars x y z # declare your variables\n",
    "g(x,y,z)=exp(x*y*z)+sin(x/y) # make a gnarly function\n",
    "diff(g(x,y,z),z) #finds the partial derivative with respect to z\n",
    "                 # you can do the same with x and y. See below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e0121a785bfadbb8efd4a683c6597d0",
     "grade": false,
     "grade_id": "cell-91cad2601b647040",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### diff(f(x, y, z), x)  will return an expression representing the gradient in the x direction for any (x, y, z)\n",
    "\n",
    "Using SymPy, find the three expressions for df/dx, df/dy, and df/dz.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "300a918575c4a10805876dd00cc72003",
     "grade": false,
     "grade_id": "cell-a61ef91dc1600417",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Use SymPy to find an expression for df/dx\n",
    "#here is f declared again, if you need it\n",
    "f(x, y, z) = x*y*exp(z) - 14y*z + sin(x)*cos(y) - exp((tan(z/y)))\n",
    "# your code here\n",
    "throw(ErrorException())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a235eb7d0127b45b0b6b10804d8ef88",
     "grade": false,
     "grade_id": "cell-1fde668edf8c7f52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#use SymPy to find an expression for df/dy\n",
    "# your code here\n",
    "throw(ErrorException())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "568dc5ae9ba7b1131374d07b83589756",
     "grade": false,
     "grade_id": "cell-1171f72d3b289d08",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#use SymPy to find an expression for df/dz\n",
    "# your code here\n",
    "throw(ErrorException())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd8ac31db6b056ce5de38f486f64084e",
     "grade": false,
     "grade_id": "cell-98028d296ba73173",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "throw(ErrorException())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42ca894c345e11dabc427b88f8575336",
     "grade": true,
     "grade_id": "cell-16019977add57efe",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your implementation of Grad should return the same answer as you got in problem 1\n",
    "@show grad_at_1 = Grad(-1, -1.1, 1)\n",
    "@assert isapprox(gradVect, grad_at_1, atol = 1E-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82046b82504323c6a0491a10d2e124ed",
     "grade": false,
     "grade_id": "cell-67c31e69f2dc1518",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Example:  The Hessian\n",
    "A reminder that the Hessian is the Jacobian of the transpose of the gradient of a function $f$: is that a mouthful or what? Why do we care? Extrema of a function (that is, max, min, and something else called a saddle point) \"live\" in the zero set of $ \\nabla f(x)$. Hence, if $x^\\ast$ is a local minimun of $f$, it satisfies $\\nabla f(x^\\ast)=0$, which means it is a root of $\\nabla f(x)=0$. We can therefore apply Newton-Raphson to the gradient function in order to find its roots, and hence find local minima, for example. \n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\label{eq:Hessian}\n",
    "    \\nabla^2 f(x) := \\frac{\\partial}{\\partial x} \\left[ \\nabla f(x) \\right]^\\top\n",
    "\\end{equation}$$\n",
    "Where $\\nabla^2 f(x) $ is the Hessian of $f$ at point $x$.\n",
    "\n",
    "The following function uses a symmetric differences approximation to compute the Hessian of f at x0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c62bc84e71790ccb1cfc680168e6c403",
     "grade": false,
     "grade_id": "cell-13883fef66cc8507",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run me, don't change me. I will compute gradients and Hessians for you. \n",
    "# I am a workhorse of a function!\n",
    "#\n",
    "function gradHess(f,x0) \n",
    "    n=size(x0,1)\n",
    "    H=zeros(n,n)\n",
    "    grad1=zeros(1,n)\n",
    "    Id=diagm(0=>fill(1., size(H,1)))\n",
    "    delta=0.01\n",
    "    h=delta\n",
    "    for i=1:n\n",
    "        grad1[i]=(f(x0+ h*Id[:,i]) -f(x0 -  h*Id[:,i]))[1]/(2*h)\n",
    "        for j=1:n\n",
    "            H[i,j]=(f(x0+ h*Id[:,i] + delta*Id[:,j])-  f(x0+  h*Id[:,i]-delta*Id[:,j]) - f(x0-  h*Id[:,i] + delta*Id[:,j])+ f(x0- h*Id[:,i]-delta*Id[:,j]))[1]/(4*h*delta)\n",
    "        end\n",
    "    end\n",
    "    return  grad1, H\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58602c25b5add5740da12b92ca577972",
     "grade": false,
     "grade_id": "cell-4b22a267eb3564be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run me, don't change me. I am creating a super complicated function to show you\n",
    "# how efficiently gradHess can compute the gradient and the Hessian. I wish you luck\n",
    "# doing this anlytically! Go for it, make my day!  \n",
    "#\n",
    "#f and x0 are declared to be random matrices/vectors of size 20 \n",
    "using LinearAlgebra\n",
    "using Random\n",
    "Random.seed!(4321);\n",
    "n=20;\n",
    "A2=rand(n,n);\n",
    "A4=rand(n,n);\n",
    "f(x)= x'*A2*x + x'*x*x'*A4*x;\n",
    "x0=rand(n,1)\n",
    "#Here is in example using the Hessian function \n",
    "(gradN, Hess) =gradHess(f, x0)\n",
    "@show gradN\n",
    "Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3:  Use gradHess to minimize a function g(x), where $x\\in \\mathbb{R}^{20}$\n",
    "Please re-read Section 11.4 in our textbook. A few key points are summarized here, but for the full context, please see the book! We have $g:\\mathbb{R}^n \\to \\mathbb{R}$. We seek\n",
    "$$x^\\ast = {\\rm arg~min}_{x\\in \\mathbb{R}^n} g(x) $$\n",
    "We know that $x^\\ast$ is a root of $\\nabla g(x)$. Because the gradient is a row vector, we take its transpose and turn it into a column vector so that\n",
    "$$ \\nabla g^\\top: \\mathbb{R}^n \\to \\mathbb{R}^n.$$\n",
    "We know how to find roots of vector-valued functions: we must apply Newton-Raphson to the function. To do that, we need the Jacobian of $\\nabla g^\\top(x)$, which is the Hessian. Yes, $\\nabla^2g(x):= \\frac{\\partial }{\\partial x }\\nabla g^\\top(x).$\n",
    "\n",
    "Lucky you, we have provided free of charge the awesome function:\n",
    "\n",
    "**(grad, Hess) = gradHess(g, x0)**\n",
    "\n",
    "which computes both the gradient and the Hessian. You'll have to transpose the gradient yourself! We think you can handle it. Your mission therefore, is to implement the algorithm below on a gnarly function $g:\\mathbb{R}^n \\to \\mathbb{R}$ and find its minimum to a tolerance of ${\\rm tol}=10^{-6}$!\n",
    "### Newton-Raphson applied to the transpose of the gradient so that a local minimum can be found: You must put this in some kind of loop(For or While), just as you did in HW for the Bisection Algorithm. \n",
    "### In a loop, solve the top equation for $\\Delta x_{k} $ and then use it to update the second equation.\n",
    "$$\n",
    "\\nabla^2 g(x_k)~ \\Delta x_{k} = - \\left[\\nabla g(x_k) \\right]^\\top\n",
    "$$\n",
    "$$\n",
    "x_{k+1}= x_k + \\Delta x_{k}\n",
    "$$\n",
    "#### Exit the loop when $||\\nabla g(x_k)^\\top|| < {\\rm tol} $.\n",
    "g(x) and an initial xk for k=0 are declared for you.  You may find that page 188 of the booklet is helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e78a1970e7dfaf2980e9484aa3bdef3",
     "grade": false,
     "grade_id": "cell-710cd9286f1793f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run me, don't change me.\n",
    "# this cell declares g(x), a function that depends on 20 variables, and xk for k=0, \n",
    "# which you are to use as your starting point\n",
    "n=20 \n",
    "Random.seed!(4321); \n",
    "A2=rand(n,n) \n",
    "A4=rand(n,n) \n",
    "g(x)= -x'*A2'*A2*x + x'*x*x'*A4'*A4*x\n",
    "k=0\n",
    "xk=100*rand(n,1)-200*rand(n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b2b25334ed3981cf3903fb3f39ab117",
     "grade": false,
     "grade_id": "cell-1366d570762336ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Use this cell to find the minimum\n",
    "#Save the value of the minimizer, x*, in a variable called x_star\n",
    "#Save the minumum value of g in a variable called g_min\n",
    "#Use the gradHess function that we provided to calculate the Hessian\n",
    "# your code here\n",
    "throw(ErrorException())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c72cb51baa67c0e6eabd1ae740669fd6",
     "grade": true,
     "grade_id": "cell-248880201bfa547e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#autograder cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc07d0ffbc54abd8911f5d39b0b3ae02",
     "grade": true,
     "grade_id": "cell-b6bf0c1e64a49cb7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#autograder cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
